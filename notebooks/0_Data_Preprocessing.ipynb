{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-12T10:58:32.224730Z",
     "iopub.status.busy": "2024-06-12T10:58:32.224336Z",
     "iopub.status.idle": "2024-06-12T10:58:32.240107Z",
     "shell.execute_reply": "2024-06-12T10:58:32.239343Z",
     "shell.execute_reply.started": "2024-06-12T10:58:32.224693Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script src=\"require.js\"></script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<script src=\"require.js\"></script>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-12T10:58:33.851740Z",
     "iopub.status.busy": "2024-06-12T10:58:33.850911Z",
     "iopub.status.idle": "2024-06-12T10:58:33.859480Z",
     "shell.execute_reply": "2024-06-12T10:58:33.858611Z",
     "shell.execute_reply.started": "2024-06-12T10:58:33.851703Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <script\n",
       "        src='https://cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js'>\n",
       "    </script>\n",
       "    <script>\n",
       "        code_show=true;\n",
       "        function code_toggle() {\n",
       "        if (code_show){\n",
       "        $('div.jp-CodeCell > div.jp-Cell-inputWrapper').hide();\n",
       "        } else {\n",
       "        $('div.jp-CodeCell > div.jp-Cell-inputWrapper').show();\n",
       "        }\n",
       "        code_show = !code_show\n",
       "        }\n",
       "        $( document ).ready(code_toggle);\n",
       "    </script>\n",
       "    <form action='javascript:code_toggle()'>\n",
       "        <input type=\"submit\" value='Click here to toggle on/off the raw code.'>\n",
       "    </form>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML(\n",
    "    \"\"\"\n",
    "    <script\n",
    "        src='https://cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js'>\n",
    "    </script>\n",
    "    <script>\n",
    "        code_show=true;\n",
    "        function code_toggle() {\n",
    "        if (code_show){\n",
    "        $('div.jp-CodeCell > div.jp-Cell-inputWrapper').hide();\n",
    "        } else {\n",
    "        $('div.jp-CodeCell > div.jp-Cell-inputWrapper').show();\n",
    "        }\n",
    "        code_show = !code_show\n",
    "        }\n",
    "        $( document ).ready(code_toggle);\n",
    "    </script>\n",
    "    <form action='javascript:code_toggle()'>\n",
    "        <input type=\"submit\" value='Click here to toggle on/off the raw code.'>\n",
    "    </form>\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Title'></a>\n",
    "<h1 style=\"color:#000000; background-color:#f7b731; padding: 20px 0; text-align: center; font-weight: bold;\">Data Preprocessing</h1><a id='ExecSum'></a><a id='Title'></a><a id='Title'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook serves the purpose of transforming the raw data into a merged and preprocessed data repository which will then be used for analysis and training ML models. This process involves **standardizing** and **transforming** the raw data from various taxi datasets and **merging** them into strategically **partitioned Parquet files** which are written to storage. This approach frontloads the computational expense, which reduces further computational costs downstream by eliminating repetitive processing tasks. Additionally, it leads to lower memory usage and optimized querying and reading capabilities. \n",
    "\n",
    "The output is a table stored in Parquet files collectively containing 32.2 million data points. This was achieved after transforming and aggregating the original 1.9 billion raw data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## About the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The orginal data source used for this project is New York City (NYC) Taxi and Limousine Commission (TLC) Trip Record Data for Yellow Taxis, Green Taxis, For-Hire Vehicle (“FHV”) and High Volume For-Hire Vehicle (“FHVHV”). The data was accessed from the AWS Marketplace as part of the AWS Open Data Sponsorship Program. \n",
    "\n",
    "The original data came from four main datasets:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Source | Description                                                                                                                                                                        |\n",
    "|----------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| Yellow   | Traditionally hailed by signaling to a driver who is on duty and seeking a passenger (street hail), but now they may also be hailed using an e-hail app like Curb or Arro. Yellow taxis are the only vehicles permitted to respond to a street hail from a passenger in all five boroughs. |\n",
    "| Green    | Also known as boro taxis and street-hail liveries. Green taxis may respond to street hails, but only in the areas outside of lower Manhattan.                                        |\n",
    "| FHV      | These are smaller-volume for-hire vehicle bases (Uber Pool, Lyft Line), community livery bases, luxury limousine bases, and black car bases.                                         |\n",
    "| FHVHV    | These are high-volume for-hire vehicle bases (bases for companies dispatching 10,000+ trip per day, meaning Uber, Lyft, Via, and Juno), community livery bases, luxury limousine bases, and black car bases.                          |\n",
    "<center><b>Table 1. </b> Dataset Description</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Each of the datasets were originally partitioned into separate Parquet files corresponding to a unique date for that source. These were not unified but separate, meaning the same date was often repeated four times for each source. Each row of each file represented a unique trip, with features containing details about the trip. The column names and datatypes were inconsistent and had to made consistent. The table below describes the general structure of the relevant columns for this project using aliases and the casted data type.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "| Alias        | Data Type (Casted) | Description                                                      |\n",
    "|--------------|--------------------|------------------------------------------------------------------|\n",
    "| pu_id        | float64            | The ID of the Pick-Up location.                                  |\n",
    "| do_id        | float64            | The ID of the Drop-Off location.                                 |\n",
    "| pu_datetime  | timestamp          | The timestamp of pick-up, containing the precise date and time.  |\n",
    "| do_datetime  | timestamp          | The timestamp of drop-off, containing the precise date and time. |\n",
    "\n",
    "<center><b>Table 2. </b> Relevant Column Aliases</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sets up the Spark context and imports the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.install_pypi_package('pandas')\n",
    "sc.install_pypi_package('numpy')\n",
    "sc.install_pypi_package('pyspark')\n",
    "sc.install_pypi_package('pyarrow')\n",
    "sc.install_pypi_package('boto3')\n",
    "sc.install_pypi_package('s3fs')\n",
    "sc.install_pypi_package('holidays')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import boto3\n",
    "import s3fs\n",
    "\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "import pyspark.pandas as ps\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, DoubleType,\n",
    "    StringType, TimestampType, IntegerType\n",
    ")\n",
    "from pyspark.sql.window import Window\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"s3.amazonaws.com\")\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.path.style.access\", \"true\")\n",
    "spark._jsc.hadoopConfiguration().set(\n",
    "    \"fs.s3a.access.key\", \"****\")\n",
    "spark._jsc.hadoopConfiguration().set(\n",
    "    \"fs.s3a.secret.key\", \"*****\")\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "s3 = boto3.client('s3')\n",
    "s3fs_ = s3fs.S3FileSystem(anon=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Title'></a>\n",
    "<h1 style=\"color:#000000; background-color:#f7b731; padding: 20px 0; text-align: center; font-weight: bold;\">Functions</h1><a id='ExecSum'></a><a id='Title'></a><a id='Title'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Function | Description |\n",
    "|----------|----------------|\n",
    "| get_filesnames | Retrieves all the filenames for a given vendor and time window.|\n",
    "| is_holiday (UDF) | Accepts a date and returns 1 if it is a US holiday, 0 if not.|\n",
    "| preproc_df | Filters out missing data of the df and extracts time-related features.|\n",
    "<center><b>Table 3. </b> Functions Overview</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import holidays\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "\n",
    "def get_filenames(vendor, years=range(2015, 2024)):\n",
    "    \"\"\"Retrieve file names for a given vendor across specified years.\"\"\"\n",
    "    files = []\n",
    "    for year in years:\n",
    "        prefix = f\"trip data/{vendor}_tripdata_{year}-\"\n",
    "        response = s3.list_objects_v2(Bucket='nyc-tlc', Prefix=prefix)\n",
    "        if 'Contents' in response:\n",
    "            files.extend([obj['Key'] for obj in response['Contents']\n",
    "                          if obj['Key'].endswith(\".parquet\")])\n",
    "    return files\n",
    "\n",
    "\n",
    "def is_holiday(date):\n",
    "    \"\"\"Check if a given date is a US holiday.\"\"\"\n",
    "    us_holidays = holidays.US(years=range(2012, 2023))\n",
    "    return 1 if date in us_holidays else 0\n",
    "\n",
    "\n",
    "is_holiday_udf = udf(is_holiday, IntegerType())\n",
    "\n",
    "\n",
    "def preproc_df(df):\n",
    "    \"\"\"Preprocess dataframe to clean and aggregate data.\"\"\"\n",
    "    df = (df.filter(\n",
    "        (F.col(\"pu_id\").isNotNull()) &\n",
    "        (F.col(\"do_id\").isNotNull()) &\n",
    "        (F.col(\"pu_datetime\").isNotNull()) &\n",
    "        (F.col(\"do_datetime\").isNotNull()) &\n",
    "        (~F.col(\"pu_id\").isin([264, 265, 0])) &\n",
    "        (~F.col(\"do_id\").isin([264, 265, 0]))\n",
    "    )\n",
    "        .withColumn(\"pu_datetime\", F.date_trunc(\"hour\", F.col(\"pu_datetime\")))\n",
    "        .groupBy(\"pu_id\", \"pu_datetime\")\n",
    "        .agg(F.count(\"*\").alias(\"count\"))\n",
    "        .withColumn(\"date_day\", F.to_date(F.col(\"pu_datetime\")))\n",
    "        .withColumn(\"DayOfWeek\", F.date_format(F.col(\"pu_datetime\"), \"E\"))\n",
    "        .withColumn(\"Month\", F.month(F.col(\"pu_datetime\")))\n",
    "        .withColumn(\"HourOfDay\", F.hour(F.col(\"pu_datetime\")))\n",
    "        .withColumn(\"Year\", F.year(F.col(\"pu_datetime\")))\n",
    "        .withColumn(\"is_Holiday\", is_holiday_udf(F.col(\"date_day\").cast(\n",
    "            \"date\")))\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Title'></a>\n",
    "<h1 style=\"color:#000000; background-color:#f7b731; padding: 20px 0; text-align: center; font-weight: bold;\">Data Cleaning</h1><a id='ExecSum'></a><a id='Title'></a><a id='Title'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step involves cleaning the raw data by filtering incomplete data and standardizing the column names and data types. The raw data of each vendor is read separately, and then made to conform to a new common schema. The outputs are intermediary tables written to storage due to their high memory costs. Some partial aggregation also occurs here to reduce the output size. \n",
    "\n",
    "PyArrow was used to perform this task for the following reasons:\n",
    "\n",
    "* **Type Safety and Consistency**: PyArrow validates and converts data types to maintain consistency and reduce runtime errors in downstream processing.\n",
    "\n",
    "* **Schema Evolution**: PyArrow facilitates schema evolution, allowing updates like adding new columns or modifying existing ones without losing compatibility with older data versions.\n",
    "\n",
    "* **Efficient Null Handling**: PyArrow efficiently manages null values to prevent errors and ensure data integrity, even in large datasets with missing information.\n",
    "\n",
    "* **Writing Parquet Files**: PyArrow writes Parquet files with a consistent schema and detailed metadata, enhancing stability and reliability for subsequent data usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-08T07:16:13.844519Z",
     "iopub.status.busy": "2024-06-08T07:16:13.844196Z",
     "iopub.status.idle": "2024-06-08T07:16:13.849710Z",
     "shell.execute_reply": "2024-06-08T07:16:13.848827Z",
     "shell.execute_reply.started": "2024-06-08T07:16:13.844492Z"
    }
   },
   "outputs": [],
   "source": [
    "cols_map = {\n",
    "            'green': {'pu_id': \"PULocationID\",\n",
    "                      'do_id': \"DOLocationID\",\n",
    "                      'pu_datetime': \"lpep_pickup_datetime\",\n",
    "                      'do_datetime': \"lpep_dropoff_datetime\"},\n",
    "            'yellow': {'pu_id': \"PULocationID\",\n",
    "                       'do_id': \"DOLocationID\",\n",
    "                       'pu_datetime': \"tpep_pickup_datetime\",\n",
    "                       'do_datetime': \"tpep_dropoff_datetime\"},\n",
    "            'fhv': {'pu_id': \"PUlocationID\",\n",
    "                   'do_id': \"DOlocationID\",\n",
    "                   'pu_datetime': \"pickup_datetime\",\n",
    "                   'do_datetime': \"dropOff_datetime\"},\n",
    "            'fhvhv': {'pu_id': \"PULocationID\",\n",
    "                   'do_id': \"DOLocationID\",\n",
    "                   'pu_datetime': \"pickup_datetime\",\n",
    "                   'do_datetime': \"dropoff_datetime\"},\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vendors = ['yellow', 'green', 'fhv', 'fhvhv']\n",
    "years = range(2015, 2024)\n",
    "\n",
    "for vendor in vendors:\n",
    "    files = get_filenames(vendor, years)\n",
    "    cols = list(cols_map[vendor].values())\n",
    "    pu_id, do_id, pickup_datetime, dropoff_datetime = cols\n",
    "    schema_map = {\n",
    "        pu_id: pa.float64(),\n",
    "        do_id: pa.float64(),\n",
    "        pickup_datetime: pa.timestamp('us'),\n",
    "        dropoff_datetime: pa.timestamp('us')\n",
    "    }\n",
    "\n",
    "    for file in files:\n",
    "        file_path = f\"s3://nyc-tlc/{file}\"\n",
    "        table = pq.read_table(file_path, filesystem=s3fs_).select(cols)\n",
    "\n",
    "        new_fields = []\n",
    "        for field in table.schema:\n",
    "            dtype = schema_map.get(field.name, field.type)\n",
    "            if dtype == pa.string():\n",
    "                new_fields.append(pa.field(field.name, pa.string()))\n",
    "            elif dtype == pa.timestamp('us'):\n",
    "                new_fields.append(pa.field(field.name, pa.timestamp('us')))\n",
    "            else:\n",
    "                new_fields.append(pa.field(field.name, pa.float64()))\n",
    "        new_schema = pa.schema(new_fields)\n",
    "\n",
    "        df = table.to_pandas()\n",
    "        for column, dtype in schema_map.items():\n",
    "            if column in df.columns:\n",
    "                if dtype == pa.string():\n",
    "                    df[column] = df[column].fillna('').astype(str)\n",
    "                elif dtype != pa.timestamp('us'):\n",
    "                    df[column] = pd.to_numeric(df[column], errors='coerce')\n",
    "\n",
    "        table = pa.Table.from_pandas(df, schema=new_schema)\n",
    "\n",
    "        df = table.to_pandas()\n",
    "        df.columns = ['pu_id', 'do_id', 'pu_datetime', 'do_datetime']\n",
    "        df = preproc_df(df)\n",
    "        folder = file.replace(\"trip data/\", \"\").replace(\".parquet\", \"\")\n",
    "        df.write.mode('overwrite').parquet(\n",
    "            f\"s3://<personal bucket>/agg/{folder}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Title'></a>\n",
    "<h1 style=\"color:#000000; background-color:#f7b731; padding: 20px 0; text-align: center; font-weight: bold;\">Feature Extraction</h1><a id='ExecSum'></a><a id='Title'></a><a id='Title'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the cleaned tables from the previous step, this step performs feature extraction and again writes these intermediary tables to storage rather than retaining the data in memory due to limitations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filenames_s3(vendor, years=range(2015, 2024)):\n",
    "    \"\"\"\n",
    "    Retrieve a list of Parquet file names from an S3 bucket for a specified \n",
    "    vendor and range of years.\n",
    "\n",
    "    Parameters:\n",
    "    vendor (str): The vendor name to include in the file search prefix.\n",
    "    years (iterable, optional): A range or list of years to search for files.\n",
    "    Defaults to range(2015, 2024).\n",
    "\n",
    "    Returns:\n",
    "    list: A list of strings, where each string is the key of a Parquet file\n",
    "    stored in the S3 bucket.\n",
    "    \"\"\"\n",
    "    files = []\n",
    "    for year in years:\n",
    "        prefix = f\"agg/{vendor}_tripdata_{year}-\"\n",
    "        response = s3.list_objects_v2(\n",
    "            Bucket='<personal bucket>', Prefix=prefix)\n",
    "        if 'Contents' in response:\n",
    "            files.extend([obj['Key'] for obj in response['Contents']\n",
    "                         if obj['Key'].endswith(\".parquet\")])\n",
    "    return files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below reads the data from each vendor separately and performs feature extraction. The intermediary tables are written to storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vendors = ['yellow', 'green', 'fhvhv', 'fhv']\n",
    "s3_path = \"s3://<personal bucket>/\"\n",
    "\n",
    "for vendor in vendors:\n",
    "    file_names = get_filenames_s3(vendor)\n",
    "    for file in file_names:\n",
    "        file_path = s3_path + file\n",
    "        out_path = file_path.replace(\"/agg/\", \"/nyc_trips/\")\n",
    "        table = pq.read_table(file_path)\n",
    "        table = table.set_column(\n",
    "            table.schema.get_field_index('pu_datetime'), pa.field(\n",
    "                'pu_datetime', pa.timestamp('ms')),\n",
    "            table.column('pu_datetime').cast(pa.timestamp('ms'))\n",
    "        ).set_column(\n",
    "            table.schema.get_field_index(\n",
    "                'pu_id'), pa.field('pu_id', pa.float64()),\n",
    "            table.column('pu_id').cast(pa.float64())\n",
    "        ).set_column(\n",
    "            table.schema.get_field_index(\n",
    "                'count'), pa.field('count', pa.int64()),\n",
    "            table.column('count').cast(pa.int64())\n",
    "        ).set_column(\n",
    "            table.schema.get_field_index(\n",
    "                'date_day'), pa.field('date_day', pa.date32()),\n",
    "            table.column('date_day').cast(pa.date32())\n",
    "        ).set_column(\n",
    "            table.schema.get_field_index('DayOfWeek'), pa.field(\n",
    "                'DayOfWeek', pa.string()),\n",
    "            table.column('DayOfWeek').cast(pa.string())\n",
    "        ).set_column(\n",
    "            table.schema.get_field_index(\n",
    "                'Month'), pa.field('Month', pa.int32()),\n",
    "            table.column('Month').cast(pa.int32())\n",
    "        ).set_column(\n",
    "            table.schema.get_field_index(\n",
    "                'HourOfDay'), pa.field('HourOfDay', pa.int32()),\n",
    "            table.column('HourOfDay').cast(pa.int32())\n",
    "        ).set_column(\n",
    "            table.schema.get_field_index('Year'), pa.field(\n",
    "                'Year', pa.int32()),\n",
    "            table.column('Year').cast(pa.int32())\n",
    "        ).set_column(\n",
    "            table.schema.get_field_index('is_Holiday'), pa.field(\n",
    "                'is_Holiday', pa.int32()),\n",
    "            table.column('is_Holiday').cast(pa.int32())\n",
    "        )\n",
    "\n",
    "        pq.write_table(table, out_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging and Writing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data can now be read collectively into a unified Spark dataframe containing all the necessary information for downstream steps. The resulting table is then written to storage in Parquet files which are partitioned by `pu_id` and `HourOfDay`. \n",
    "\n",
    "The partitions were chosen as the reading and querying actions of downstream steps will extensively use those fields for identifying relevant subsets of data. This was found to be the optimal balance as overly granular partitioning can lead to an excessive number of small files, which can degrade performance due to increased overhead in file management and metadata processing. Conversely, overly broad partitioning can result in larger files that contain more irrelevant data, increasing the I/O and processing time required for queries that do not require the full dataset. This trade-off involves balancing file management efficiency and query performance. \n",
    "\n",
    "The resulting table can now be written to storage to be used for performing analysis, training a hybrid ML model, and developing predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\n",
    "    \"s3://<personal bucket>/*/*.parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = 's3://<personal bucket>/trips0/'\n",
    "df.write.partitionBy('pu_id', 'HourOfDay') \\\n",
    "    .parquet(out_path, mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Title'></a>\n",
    "<h1 style=\"color:#000000; background-color:#f7b731; padding: 20px 0; text-align: center; font-weight: bold;\">References</h1><a id='ExecSum'></a><a id='Title'></a><a id='Title'></a>\n",
    "\n",
    "[AWS Marketplace: New York City Taxi and Limousine Commission (TLC) Trip Record Data](https://aws.amazon.com/marketplace/pp/prodview-okyonroqg5b2u#links) (accessed June 1, 2024)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
